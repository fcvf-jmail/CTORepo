{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flappy Bird AI на Q-Learning\n",
        "\n",
        "В этой тетради мы создаём полностью автоматизированного агента, который учится играть в Flappy Bird при помощи Q-Learning и нейросети Linear QNet. Для наглядности ноутбук разбит на логические секции, а каждая строка кода снабжена подробными комментариями.\n",
        "\n",
        "**Структура ноутбука:**\n",
        "1. Импорт библиотек и подготовка среды выполнения.\n",
        "2. Конфигурация игры и вспомогательные структуры данных.\n",
        "3. Реализация Flappy Bird на pygame (физика, препятствия, визуализация и состояние).\n",
        "4. Нейронная сеть Linear QNet, тренер Q-Learning и визуализация метрик.\n",
        "5. Агент с памятью опыта, epsilon-greedy стратегией и функцией обучения.\n",
        "6. Запуск цикла обучения на 200 играх, сохранение лучшей модели и построение графиков.\n",
        "7. Генерация GIF с игрой обученного ИИ через create_ai_gif.\n",
        "\n",
        "Все пояснения, описания параметров и комментарии приведены на русском языке и соответствуют требованиям задания.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Коротко о Q-Learning и гиперпараметрах\n",
        "\n",
        "Обновление оценки качества действия выполняется по формуле:\n",
        "\n",
        "**Q_new = Q_old + alpha * (reward + gamma * max(Q_next) - Q_old)**\n",
        "\n",
        "Где:\n",
        "- *state* и *next_state* — векторы из 11 признаков (опасности, направление, цель).\n",
        "- *action* — одно из трёх действий (ничего, взмах, пикирование).\n",
        "- *alpha (learning rate)* — скорость обучения оптимизатора Adam.\n",
        "- *gamma* — коэффициент дисконтирования будущих наград.\n",
        "\n",
        "Linear QNet получает 11 входов, имеет скрытый слой на 256 нейронов и три выхода. Память опыта на 100 000 переходов реализует Experience Replay, а epsilon-greedy стратегия постепенно снижает долю случайных действий от 200 до 5 по мере прохождения 200 игр.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os  # Импортируем модуль os для управления системными переменными окружения\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"  # Отключаем аудиодрайвер pygame, чтобы запуск проходил без звука\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # Включаем виртуальный видеодрайвер, чтобы окно pygame работало без физического дисплея\n",
        "import random  # Используем random для стохастики среды и стратегии epsilon-greedy\n",
        "from collections import deque  # deque обеспечивает быструю реализацию памяти опыта с ограниченным размером\n",
        "from dataclasses import dataclass  # dataclass используется для компактного описания конфигураций и структур данных\n",
        "from typing import Deque, List, Tuple  # Импортируем типы для повышения читаемости и статической проверки\n",
        "import numpy as np  # NumPy нужен для работы с векторами состояний и преобразованиями\n",
        "import pygame  # pygame отвечает за визуализацию и игровую механику Flappy Bird\n",
        "import torch  # PyTorch используется для реализации нейросети агента\n",
        "import torch.nn as nn  # Подмодуль nn содержит готовые слои и функции активации\n",
        "import torch.optim as optim  # Подмодуль optim предоставляет оптимизаторы, включая Adam\n",
        "from IPython import display as ipy_display  # display позволяет обновлять графики внутри Jupyter Notebook\n",
        "import matplotlib.pyplot as plt  # Matplotlib используем для построения кривых очков и среднего значения\n",
        "import imageio  # ImageIO понадобится для сохранения последовательности кадров в GIF\n",
        "plt.style.use(\"seaborn-v0_8\")  # Задаём стиль графиков, чтобы визуализация была читабельной с первого запуска\n",
        "plt.ion()  # Включаем интерактивный режим Matplotlib для динамического обновления графиков\n",
        "pygame.init()  # Инициализируем основной модуль pygame сразу после настройки драйверов\n",
        "pygame.font.init()  # Отдельно активируем подсистему шрифтов для отображения текста на экране\n",
        "random.seed(42)  # Фиксируем сид генератора случайных чисел Python для воспроизводимости\n",
        "np.random.seed(42)  # Повторяем то же самое для NumPy, чтобы состояние среды было детерминировано\n",
        "torch.manual_seed(42)  # Аналогично инициализируем сид PyTorch для повторяемого обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "@dataclass  # Используем dataclass, чтобы удобно группировать все параметры игры и обучения\n",
        "class GameConfig:  # Класс GameConfig хранит значения, контролирующие физику, визуализацию и обучение\n",
        "    screen_width: int = 288  # Ширина окна Flappy Bird в пикселях\n",
        "    screen_height: int = 512  # Высота окна игры\n",
        "    fps: int = 60  # Количество кадров в секунду\n",
        "    gravity: float = 0.5  # Постоянная гравитации, которая притягивает птицу вниз каждый кадр\n",
        "    flap_strength: float = 9.5  # Скорость, которую получает птица при взмахе крыльями\n",
        "    fast_drop_force: float = 3.0  # Дополнительное ускорение вниз при действии «пикирование»\n",
        "    max_fall_speed: float = 12.0  # Ограничение на максимальную скорость падения\n",
        "    pipe_gap: int = 140  # Высота отверстия между верхней и нижней трубой\n",
        "    pipe_width: int = 52  # Ширина одной трубы\n",
        "    pipe_distance: int = 200  # Минимальное расстояние между соседними трубами по оси X\n",
        "    pipe_speed: int = 3  # Скорость сдвига труб влево каждый кадр\n",
        "    bird_x: int = 50  # Горизонтальная позиция птицы\n",
        "    bird_width: int = 34  # Ширина прямоугольника птицы\n",
        "    bird_height: int = 24  # Высота прямоугольника птицы\n",
        "    background_color: Tuple[int, int, int] = (135, 206, 235)  # Цвет неба в формате RGB\n",
        "    pipe_color: Tuple[int, int, int] = (34, 139, 34)  # Цвет труб (тёмно-зелёный)\n",
        "    bird_color: Tuple[int, int, int] = (255, 255, 0)  # Цвет птицы (жёлтый прямоугольник)\n",
        "    text_color: Tuple[int, int, int] = (0, 0, 0)  # Цвет текста интерфейса (чёрный)\n",
        "    alive_reward: float = 0.1  # Малое положительное вознаграждение за каждый прожитый кадр\n",
        "    pipe_reward: float = 10.0  # Награда за успешное прохождение очередной трубы\n",
        "    collision_penalty: float = -10.0  # Штраф за столкновение с трубой или выход за границы\n",
        "    max_memory: int = 100_000  # Объём памяти опыта для Experience Replay\n",
        "    batch_size: int = 1000  # Размер батча для обучения из памяти опыта\n",
        "    lr: float = 0.001  # Скорость обучения оптимизатора Adam\n",
        "    gamma: float = 0.9  # Коэффициент дисконтирования будущих наград\n",
        "    max_games: int = 200  # Количество игр, которые проведём во время тренировки\n",
        "    exploration_start: float = 200.0  # Начальное значение epsilon для стратегии epsilon-greedy\n",
        "    exploration_end: float = 5.0  # Минимальное значение epsilon, чтобы всегда оставлять немного исследования\n",
        "    exploration_decay: float = 1.0  # Значение, на которое уменьшается epsilon после каждой игры\n",
        "    best_model_path: str = \"best_flappy_q_agent.pth\"  # Путь для сохранения лучшей модели агента\n",
        "    gif_path: str = \"flappy_q_learning.gif\"  # Путь для итогового GIF с игрой ИИ\n",
        "    gif_max_steps: int = 600  # Количество кадров, записываемых в GIF, чтобы показать несколько проходов\n",
        "\n",
        "@dataclass  # Определяем структуру для характеристик трубы\n",
        "class Pipe:  # Класс Pipe хранит положение X и вертикальный центр отверстия\n",
        "    x: int  # Текущая координата X трубы\n",
        "    gap_y: int  # Вертикальный центр отверстия между верхней и нижней частью\n",
        "    scored: bool = False  # Флаг, чтобы начислять очки ровно один раз за трубу\n",
        "\n",
        "@dataclass  # Структура для элемента памяти опыта агента\n",
        "class Experience:  # Мы будем сохранять состояние, действие, награду, новое состояние и признак завершения\n",
        "    state: np.ndarray  # Состояние среды до действия (11 параметров)\n",
        "    action: List[int]  # Вектор из 3 значений (one-hot), описывающий действие агента\n",
        "    reward: float  # Вознаграждение, полученное после действия\n",
        "    next_state: np.ndarray  # Состояние после совершения действия\n",
        "    done: bool  # Флаг окончания игры после этого шага\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class FlappyBirdGame:  # Класс, который инкапсулирует всю игровую механику Flappy Bird\n",
        "    def __init__(self, config: GameConfig):  # Конструктор принимает объект конфигурации\n",
        "        self.config = config  # Сохраняем конфигурацию для дальнейшего использования\n",
        "        self.screen = pygame.display.set_mode((config.screen_width, config.screen_height))  # Создаём окно pygame с указанными размерами\n",
        "        pygame.display.set_caption(\"Flappy Bird Q-Learning\")  # Подписываем окно, чтобы легче отслеживать симуляцию\n",
        "        self.clock = pygame.time.Clock()  # Создаём объект Clock для контроля FPS\n",
        "        self.font = pygame.font.SysFont(\"arial\", 18)  # Загружаем шрифт для отображения текста\n",
        "        self.reset()  # Инициализируем состояние игры вызовом reset\n",
        "\n",
        "    def reset(self) -> None:  # Сбрасываем игру к начальному состоянию\n",
        "        self.bird_y = self.config.screen_height // 2  # Помещаем птицу в вертикальный центр экрана\n",
        "        self.bird_velocity = 0.0  # Обнуляем вертикальную скорость\n",
        "        self.score = 0  # Сбрасываем счёт очков\n",
        "        self.frame_iteration = 0  # Обнуляем счётчик кадров\n",
        "        self.pipes: List[Pipe] = []  # Очищаем список труб\n",
        "        for _ in range(2):  # Создаём две трубы заранее\n",
        "            self.spawn_pipe(initial=True)  # Добавляем трубу с флагом initial\n",
        "\n",
        "    def spawn_pipe(self, initial: bool = False) -> None:  # Метод создаёт новую трубу и добавляет её в список\n",
        "        gap_margin = 80  # Защитные поля сверху и снизу, чтобы отверстие не смещалось в край\n",
        "        min_center = gap_margin + self.config.pipe_gap // 2  # Минимальный допустимый центр отверстия\n",
        "        max_center = self.config.screen_height - gap_margin - self.config.pipe_gap // 2  # Максимальный допустимый центр отверстия\n",
        "        gap_y = random.randint(min_center, max_center)  # Случайно выбираем положение отверстия\n",
        "        if initial and self.pipes:  # Если это начальная генерация и уже есть трубы\n",
        "            x_position = self.pipes[-1].x + self.config.pipe_distance  # Размещаем трубу на фиксированном расстоянии от предыдущей\n",
        "        elif initial:  # Если это самая первая труба\n",
        "            x_position = self.config.screen_width + 100  # Отодвигаем её вправо, чтобы игрок успел среагировать\n",
        "        else:  # В остальных случаях\n",
        "            x_position = self.config.screen_width + 20  # Размещаем трубу сразу за правой границей экрана\n",
        "        self.pipes.append(Pipe(x=x_position, gap_y=gap_y))  # Добавляем новую трубу в список препятствий\n",
        "\n",
        "    def handle_action(self, action_index: int) -> None:  # Метод применяет выбранное агентом действие\n",
        "        if action_index == 1:  # Действие «взмах» заставляет птицу резко подниматься\n",
        "            self.bird_velocity = -self.config.flap_strength  # Устанавливаем отрицательную скорость\n",
        "        elif action_index == 2:  # Действие «пикирование» ускоряет падение\n",
        "            self.bird_velocity += self.config.fast_drop_force  # Увеличиваем скорость вниз\n",
        "        else:  # Если выбран вариант «ничего не делать»\n",
        "            pass  # Птица продолжит двигаться под влиянием гравитации\n",
        "\n",
        "    def apply_physics(self) -> None:  # Метод обновляет скорость и положение птицы\n",
        "        self.bird_velocity += self.config.gravity  # Добавляем ускорение свободного падения\n",
        "        self.bird_velocity = max(-self.config.flap_strength, min(self.bird_velocity, self.config.max_fall_speed))  # Ограничиваем скорость\n",
        "        self.bird_y += self.bird_velocity  # Обновляем вертикальную позицию птицы\n",
        "\n",
        "    def move_pipes(self) -> None:  # Сдвигаем трубы влево\n",
        "        for pipe in self.pipes:  # Проходим по всем трубам\n",
        "            pipe.x -= self.config.pipe_speed  # Уменьшаем координату X\n",
        "        if self.pipes and self.pipes[0].x + self.config.pipe_width < 0:  # Если самая левая труба вышла за экран\n",
        "            self.pipes.pop(0)  # Удаляем её из списка\n",
        "        if not self.pipes or self.pipes[-1].x < self.config.screen_width - self.config.pipe_distance:  # Если нужно добавить новую трубу\n",
        "            self.spawn_pipe()  # Создаём новую трубу\n",
        "\n",
        "    def is_collision(self) -> bool:  # Проверяем столкновения\n",
        "        if self.bird_y <= 0 or self.bird_y + self.config.bird_height >= self.config.screen_height:  # Столкновение с границами\n",
        "            return True  # Сразу сообщаем о проигрыше\n",
        "        for pipe in self.pipes:  # Проверяем каждую трубу\n",
        "            bird_right = self.config.bird_x + self.config.bird_width  # Правая граница птицы\n",
        "            pipe_right = pipe.x + self.config.pipe_width  # Правая граница трубы\n",
        "            if bird_right > pipe.x and self.config.bird_x < pipe_right:  # Если по X прямоугольники пересекаются\n",
        "                gap_top = pipe.gap_y - self.config.pipe_gap // 2  # Верхняя граница отверстия\n",
        "                gap_bottom = pipe.gap_y + self.config.pipe_gap // 2  # Нижняя граница отверстия\n",
        "                if self.bird_y < gap_top or self.bird_y + self.config.bird_height > gap_bottom:  # Птица вне прохода\n",
        "                    return True  # Столкновение с трубой\n",
        "        return False  # Если все проверки пройдены, столкновения нет\n",
        "\n",
        "    def get_next_pipe(self) -> Pipe:  # Возвращаем ближайшую впереди трубу\n",
        "        for pipe in self.pipes:  # Идём по трубам слева направо\n",
        "            if pipe.x + self.config.pipe_width >= self.config.bird_x:  # Находим первую трубу, которая ещё не пройдена\n",
        "                return pipe  # Возвращаем найденную трубу\n",
        "        return self.pipes[0]  # Если все трубы позади, возвращаем самую левую\n",
        "\n",
        "    def get_state(self) -> np.ndarray:  # Формируем вектор признаков состояния\n",
        "        pipe = self.get_next_pipe()  # Получаем ближайшую трубу\n",
        "        gap_top = pipe.gap_y - self.config.pipe_gap // 2  # Верхняя точка отверстия\n",
        "        gap_bottom = pipe.gap_y + self.config.pipe_gap // 2  # Нижняя точка отверстия\n",
        "        danger_straight = int(  # Проверяем, опасна ли ближайшая труба на текущей траектории\n",
        "            pipe.x <= self.config.bird_x + self.config.bird_width + 5  # Смотрим, находится ли труба достаточно близко по горизонтали\n",
        "            and (self.bird_y < gap_top or self.bird_y + self.config.bird_height > gap_bottom)  # Проверяем, не выходит ли траектория за пределы зазора по высоте\n",
        "        )  # Преобразуем логическое выражение в 0 или 1 для признака опасности прямо\n",
        "        danger_right = int(self.bird_y <= 0)  # Опасность у потолка (аналог «справа»)\n",
        "        danger_left = int(self.bird_y + self.config.bird_height >= self.config.screen_height)  # Опасность у земли (аналог «слева»)\n",
        "        dir_up = int(self.bird_velocity < -0.1)  # Признак движения вверх\n",
        "        dir_down = int(self.bird_velocity >= -0.1)  # Признак движения вниз\n",
        "        dir_left = 0  # Горизонтального движения влево нет\n",
        "        dir_right = 1  # Птица всегда движется вправо относительно труб\n",
        "        bird_center_y = self.bird_y + self.config.bird_height / 2  # Центр птицы по вертикали\n",
        "        target_up = int(bird_center_y > pipe.gap_y)  # Цель находится выше\n",
        "        target_down = int(bird_center_y < pipe.gap_y)  # Цель находится ниже\n",
        "        target_left = int(pipe.x + self.config.pipe_width < self.config.bird_x)  # Цель позади\n",
        "        target_right = int(pipe.x + self.config.pipe_width >= self.config.bird_x)  # Цель впереди\n",
        "        state = np.array(  # Собираем упорядоченный набор бинарных признаков состояния\n",
        "            [  # Начинаем перечисление всех 11 признаков\n",
        "                danger_straight,  # Опасность прямо\n",
        "                danger_right,  # Опасность сверху\n",
        "                danger_left,  # Опасность снизу\n",
        "                dir_up,  # Направление вверх\n",
        "                dir_down,  # Направление вниз\n",
        "                dir_left,  # Направление влево\n",
        "                dir_right,  # Направление вправо\n",
        "                target_up,  # Цель выше\n",
        "                target_down,  # Цель ниже\n",
        "                target_left,  # Цель позади\n",
        "                target_right,  # Цель впереди\n",
        "            ],  # Завершаем список признаков состояния\n",
        "            dtype=int,  # Храним признаки как целые числа для совместимости с PyTorch\n",
        "        )  # Возвращаем итоговый вектор состояния\n",
        "        return state  # Возвращаем состояние агенту\n",
        "\n",
        "    def draw_pipes(self) -> None:  # Вспомогательный метод для отрисовки всех труб\n",
        "        for pipe in self.pipes:  # Проходим по списку труб\n",
        "            gap_top = pipe.gap_y - self.config.pipe_gap // 2  # Считаем верхнюю границу отверстия\n",
        "            gap_bottom = pipe.gap_y + self.config.pipe_gap // 2  # Считаем нижнюю границу отверстия\n",
        "            pygame.draw.rect(self.screen, self.config.pipe_color, pygame.Rect(pipe.x, 0, self.config.pipe_width, gap_top))  # Рисуем верхнюю часть трубы как прямоугольник\n",
        "            pygame.draw.rect(self.screen, self.config.pipe_color, pygame.Rect(pipe.x, gap_bottom, self.config.pipe_width, self.config.screen_height - gap_bottom))  # Рисуем нижнюю часть трубы с учётом свободного пространства\n",
        "\n",
        "    def draw_bird(self) -> None:  # Метод рисует птицу как прямоугольник\n",
        "        pygame.draw.rect(self.screen, self.config.bird_color, pygame.Rect(self.config.bird_x, int(self.bird_y), self.config.bird_width, self.config.bird_height))  # Отрисовываем птицу как жёлтый прямоугольник\n",
        "\n",
        "    def update_ui(self, action_index: int, game_number: int) -> None:  # Метод обновляет визуализацию и выводит текстовую информацию\n",
        "        self.screen.fill(self.config.background_color)  # Закрашиваем фон выбранным цветом неба\n",
        "        pygame.draw.rect(self.screen, self.config.bird_color, pygame.Rect(self.config.bird_x, int(self.bird_y), self.config.bird_width, self.config.bird_height))  # Рисуем птицу как жёлтый прямоугольник\n",
        "        game_surface = self.font.render(f\"Game: {game_number}\", True, self.config.text_color)  # Номер игры\n",
        "    def update_ui(self, action_index: int, game_number: int) -> None:  # Метод обновляет визуализацию и выводит текстовую информацию\n",
        "        self.screen.fill(self.config.background_color)  # Закрашиваем фон выбранным цветом неба\n",
        "        self.draw_pipes()  # Рисуем все трубы на текущем кадре\n",
        "        self.draw_bird()  # Отображаем птицу поверх труб\n",
        "        action_names = {0: \"Держим\", 1: \"Взмах\", 2: \"Пикирование\"}  # Словарь для текстового отображения действий\n",
        "        score_surface = self.font.render(f\"Score: {self.score}\", True, self.config.text_color)  # Создаём поверхность с текущим счётом\n",
        "        action_surface = self.font.render(f\"Action: {action_names[action_index]}\", True, self.config.text_color)  # Подготавливаем текст с действием\n",
        "        game_surface = self.font.render(f\"Game: {game_number}\", True, self.config.text_color)  # Текст с номером игры\n",
        "        self.screen.blit(score_surface, (10, 10))  # Показываем счёт в левом верхнем углу\n",
        "        self.screen.blit(action_surface, (10, 30))  # Размещаем текущие действия под счётом\n",
        "        self.screen.blit(game_surface, (10, 50))  # Отображаем номер игры для контроля прогресса\n",
        "        pygame.display.flip()  # Переворачиваем буфер, чтобы показать обновлённый кадр\n",
        "\n",
        "                pygame.quit()  # Корректно завершаем pygame\n",
        "                raise SystemExit(\"Окно pygame закрыто пользователем\")  # Останавливаем выполнение ноутбука\n",
        "        action_index = int(np.argmax(action_vector))  # Конвертируем one-hot действие в индекс 0/1/2\n",
        "        self.handle_action(action_index)  # Применяем действие к птице\n",
        "        self.apply_physics()  # Обновляем скорость и позицию под действием гравитации\n",
        "        self.move_pipes()  # Сдвигаем трубы\n",
        "        reward = self.config.alive_reward  # По умолчанию выдаём небольшую награду за выживание\n",
        "        done = False  # Флаг окончания игры по умолчанию\n",
        "        if self.is_collision():  # Проверяем столкновения после обновления состояний\n",
        "            reward = self.config.collision_penalty  # Начисляем штраф за проигрыш\n",
        "            done = True  # Завершаем игру\n",
        "        else:  # Если столкновения не было\n",
        "            for pipe in self.pipes:  # Проверяем, не пролетела ли птица очередную трубу\n",
        "                if not pipe.scored and pipe.x + self.config.pipe_width < self.config.bird_x:  # Если труба позади и очки ещё не начислялись\n",
        "                    pipe.scored = True  # Помечаем трубу как зачтённую\n",
        "                    self.score += 1  # Увеличиваем счёт\n",
        "                    reward = self.config.pipe_reward  # Выдаём дополнительную награду за прогресс\n",
        "                    break  # Выходим из цикла, чтобы не начислять несколько раз за кадр\n",
        "        self.frame_iteration += 1  # Увеличиваем счётчик кадров\n",
        "        self.update_ui(action_index, game_number)  # Обновляем визуализацию и текстовую информацию\n",
        "        self.clock.tick(self.config.fps)  # Ограничиваем FPS, чтобы симуляция шла с заданной скоростью\n",
        "        return reward, done, self.score  # Возвращаем награду, флаг завершения и текущий счёт\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class LinearQNet(nn.Module):  # Нейросеть с одним скрытым слоем, аппроксимирующая функцию Q\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):  # Конструктор принимает размеры слоёв\n",
        "        super().__init__()  # Вызываем инициализацию базового класса nn.Module\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)  # Первый полносвязный слой: вход (11) → скрытый уровень (256)\n",
        "        self.relu = nn.ReLU()  # Функция активации ReLU добавляет нелинейность\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)  # Второй полносвязный слой: скрытый → выход (3 действий)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # Определяем прямое распространение сигнала\n",
        "        x = self.linear1(x)  # Применяем первый линейный слой\n",
        "        x = self.relu(x)  # Передаём результаты через ReLU\n",
        "        x = self.linear2(x)  # Получаем оценки Q для каждого действия\n",
        "        return x  # Возвращаем тензор с тремя значениями Q\n",
        "\n",
        "class QTrainer:  # Класс, инкапсулирующий шаг оптимизации для Q-Learning\n",
        "    def __init__(self, model: LinearQNet, lr: float, gamma: float):  # Принимаем модель, скорость обучения и гамму\n",
        "        self.model = model  # Сохраняем ссылку на нейросеть\n",
        "        self.lr = lr  # Запоминаем скорость обучения\n",
        "        self.gamma = gamma  # Сохраняем коэффициент дисконтирования\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)  # Используем Adam для адаптивного подбора шагов\n",
        "        self.criterion = nn.MSELoss()  # В качестве функции потерь применяем MSE между целевым и предсказанным Q\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done) -> None:  # Один шаг обучения по батчу или одиночному примеру\n",
        "        state = torch.tensor(np.array(state), dtype=torch.float32)  # Преобразуем состояния в тензор с плавающей точкой\n",
        "        next_state = torch.tensor(np.array(next_state), dtype=torch.float32)  # Аналогично обрабатываем следующее состояние\n",
        "        action = torch.tensor(np.array(action), dtype=torch.float32)  # Конвертируем действия (one-hot) в тензор\n",
        "        reward = torch.tensor(np.array(reward), dtype=torch.float32)  # Награды тоже превращаем в тензор\n",
        "        if len(state.shape) == 1:  # Если на вход пришёл одиночный пример\n",
        "            state = state.unsqueeze(0)  # Добавляем измерение батча\n",
        "            next_state = next_state.unsqueeze(0)  # То же самое для следующего состояния\n",
        "            action = action.unsqueeze(0)  # И для действия\n",
        "            reward = reward.unsqueeze(0)  # А также для награды\n",
        "            done = (done,)  # Превращаем булевое значение в кортеж для унификации обработки\n",
        "        pred = self.model(state)  # Получаем предсказанные Q-значения для текущих состояний\n",
        "        target = pred.clone()  # Создаём копию, которую будем менять, формируя обучающие цели\n",
        "        for idx in range(len(done)):  # Проходим по всем элементам батча\n",
        "            q_new = reward[idx]  # Базовое значение Q — это немедленная награда\n",
        "            if not done[idx]:  # Если эпизод не завершён\n",
        "                q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))  # Добавляем дисконтированную будущую награду\n",
        "            action_index = torch.argmax(action[idx]).item()  # Определяем, какое действие было совершено\n",
        "            target[idx][action_index] = q_new  # Заменяем соответствующий элемент целевого Q новым значением\n",
        "        self.optimizer.zero_grad()  # Обнуляем градиенты перед обратным проходом\n",
        "        loss = self.criterion(pred, target)  # Вычисляем MSE между предсказанием и целями\n",
        "        loss.backward()  # Считаем градиенты с помощью backpropagation\n",
        "        self.optimizer.step()  # Обновляем веса модели\n",
        "\n",
        "scores: List[int] = []  # Список для хранения очков каждой игры\n",
        "mean_scores: List[float] = []  # Список средних значений очков по мере обучения\n",
        "\n",
        "def plot_scores(scores: List[int], mean_scores: List[float]) -> None:  # Функция строит графики Score и Mean Score\n",
        "    ipy_display.clear_output(wait=True)  # Очищаем предыдущее изображение графика\n",
        "    ipy_display.display(plt.gcf())  # Показываем текущую фигуру Matplotlib\n",
        "    plt.clf()  # Очищаем оси, чтобы перерисовать график с обновлёнными данными\n",
        "    plt.title(\"История очков Flappy Bird\")  # Заголовок графика\n",
        "    plt.xlabel(\"Номер игры\")  # Подпись оси X\n",
        "    plt.ylabel(\"Score / Mean Score\")  # Подпись оси Y\n",
        "    plt.plot(scores, label=\"Score\")  # Рисуем кривую очков каждой игры\n",
        "    plt.plot(mean_scores, label=\"Mean Score\")  # Рисуем кривую среднего значения\n",
        "    plt.legend()  # Добавляем легенду для распознавания линий\n",
        "    plt.grid(True)  # Включаем сетку для удобства чтения\n",
        "    plt.pause(0.001)  # Даём Matplotlib время обновить рисунок\n",
        "\n",
        "def create_ai_gif(model_path: str, config: GameConfig, gif_path: str, max_steps: int, duration: float = 0.06) -> None:  # Функция создаёт GIF с игрой обученного агента\n",
        "    model = LinearQNet(11, 256, 3)  # Создаём такую же архитектуру, как при обучении\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))  # Загружаем веса из сохранённого файла\n",
        "    model.eval()  # Переводим модель в режим инференса\n",
        "    game = FlappyBirdGame(config)  # Создаём новый экземпляр игры для записи\n",
        "    frames: List[np.ndarray] = []  # Список кадров, которые войдут в GIF\n",
        "    game.reset()  # Сбрасываем игру перед записью\n",
        "    done = False  # Флаг завершения эпизода\n",
        "    steps = 0  # Счётчик кадров очередного прогона\n",
        "    while not done and steps < max_steps:  # Записываем, пока игра не окончена или не достигнут лимит кадров\n",
        "        state = game.get_state()  # Получаем текущее состояние игры\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32)  # Преобразуем его в тензор\n",
        "        with torch.no_grad():  # Отключаем вычисление градиентов во время инференса\n",
        "            prediction = model(state_tensor)  # Получаем оценки Q для доступных действий\n",
        "        action_index = torch.argmax(prediction).item()  # Выбираем действие с максимальным Q\n",
        "        final_move = [0, 0, 0]  # Заготавливаем вектор one-hot\n",
        "        final_move[action_index] = 1  # Отмечаем выбранное действие как активное\n",
        "        _, done, _ = game.play_step(final_move, game_number=0)  # Делаем шаг симуляции (номер игры 0, потому что это демонстрация)\n",
        "        frame = pygame.surfarray.array3d(game.screen)  # Считываем поверхность окна pygame в виде массива\n",
        "        frame = np.transpose(frame, (1, 0, 2))  # Транспонируем оси, чтобы получить привычный формат (H, W, C)\n",
        "        frames.append(frame)  # Добавляем кадр в список\n",
        "        steps += 1  # Увеличиваем счётчик шагов\n",
        "    imageio.mimsave(gif_path, frames, duration=duration)  # Сохраняем собранные кадры в GIF с заданной скоростью\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class Agent:  # Класс, объединяющий игру, нейросеть, память и стратегию действий\n",
        "    def __init__(self, config: GameConfig):  # Конструктор принимает конфигурацию\n",
        "        self.config = config  # Сохраняем ссылку на конфигурацию\n",
        "        self.n_games = 0  # Счётчик сыгранных игр\n",
        "        self.epsilon = config.exploration_start  # Текущее значение epsilon\n",
        "        self.gamma = config.gamma  # Коэффициент дисконтирования для удобного доступа\n",
        "        self.memory: Deque[Experience] = deque(maxlen=config.max_memory)  # Создаём память опыта ограниченного размера\n",
        "        self.model = LinearQNet(11, 256, 3)  # Инициализируем нейросеть согласно требованиям\n",
        "        self.trainer = QTrainer(self.model, lr=config.lr, gamma=config.gamma)  # Создаём тренера с Adam и MSE\n",
        "        self.game = FlappyBirdGame(config)  # Создаём экземпляр игры, с которым будет взаимодействовать агент\n",
        "        self.best_score = 0  # Переменная для отслеживания максимального результата\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done) -> None:  # Сохраняем опыт в память\n",
        "        self.memory.append(Experience(state, action, reward, next_state, done))  # Добавляем пятиэлементный кортеж в deque\n",
        "\n",
        "    def train_long_memory(self) -> None:  # Обучаемся на случайном батче из памяти опыта\n",
        "        if len(self.memory) > self.config.batch_size:  # Если накоплено достаточно данных\n",
        "            mini_sample = random.sample(self.memory, self.config.batch_size)  # Берём случайный батч фиксированного размера\n",
        "        else:  # Если опыта пока мало\n",
        "            mini_sample = list(self.memory)  # Берём все доступные элементы\n",
        "        if mini_sample:  # Проверяем, что список не пуст\n",
        "            states, actions, rewards, next_states, dones = zip(*mini_sample)  # Распаковываем батч по компонентам\n",
        "            self.trainer.train_step(states, actions, rewards, next_states, dones)  # Выполняем шаг обучения на всём батче\n",
        "\n",
        "    def train_short_memory(self, state, action, reward, next_state, done) -> None:  # Обучение на одном переходе\n",
        "        self.trainer.train_step(state, action, reward, next_state, done)  # Передаём опыт напрямую тренеру\n",
        "\n",
        "    def get_action(self, state: np.ndarray) -> List[int]:  # Функция выбирает действие с учётом epsilon-greedy\n",
        "        self.epsilon = max(  # Корректируем epsilon с учётом нижней границы и скорости уменьшения\n",
        "            self.config.exploration_end,  # Минимальная доля случайных действий, чтобы не застрять в локуме\n",
        "            self.config.exploration_start - self.n_games * self.config.exploration_decay,  # Линейно уменьшаем epsilon после каждой игры\n",
        "        )  # Получаем итоговое значение epsilon для текущей игры\n",
        "        final_move = [0, 0, 0]  # Заготавливаем one-hot вектор действий\n",
        "        if random.randint(0, 200) < self.epsilon:  # С вероятностью epsilon выбираем случайное действие\n",
        "            move = random.randint(0, 2)  # Выбираем индекс действия случайным образом\n",
        "            final_move[move] = 1  # Кодируем действие в формате one-hot\n",
        "        else:  # Иначе полагаемся на нейросеть\n",
        "            state0 = torch.tensor(state, dtype=torch.float32)  # Преобразуем состояние в тензор\n",
        "            prediction = self.model(state0)  # Получаем оценки Q для каждого действия\n",
        "            move = torch.argmax(prediction).item()  # Находим действие с максимальным Q\n",
        "            final_move[move] = 1  # Записываем его в one-hot вектор\n",
        "        return final_move  # Возвращаем выбранное действие\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "config = GameConfig()  # Создаём объект конфигурации со всеми параметрами\n",
        "agent = Agent(config)  # Создаём агента, передавая ему конфигурацию\n",
        "scores = []  # Подготовим список для записи очков каждой игры\n",
        "mean_scores = []  # Список для средних значений очков\n",
        "total_score = 0  # Интегральная сумма очков для вычисления среднего\n",
        "while agent.n_games < config.max_games:  # Запускаем обучение, пока не сыграем нужное количество игр\n",
        "    old_state = agent.game.get_state()  # Снимаем состояние среды до выбора действия\n",
        "    final_move = agent.get_action(old_state)  # Получаем действие от агента (с учётом epsilon-greedy)\n",
        "    reward, done, score = agent.game.play_step(final_move, agent.n_games + 1)  # Делаем шаг в игре и получаем награду\n",
        "    new_state = agent.game.get_state()  # Снимаем новое состояние после шага\n",
        "    agent.train_short_memory(old_state, final_move, reward, new_state, done)  # Обновляем модель по одиночному опыту\n",
        "    agent.remember(old_state, final_move, reward, new_state, done)  # Сохраняем переход в память Experience Replay\n",
        "    if done:  # Если игра завершилась\n",
        "        agent.game.reset()  # Сбрасываем игровое поле\n",
        "        agent.n_games += 1  # Увеличиваем счётчик игр\n",
        "        agent.train_long_memory()  # Делаем обучение на батче из памяти\n",
        "        scores.append(score)  # Добавляем очки этой игры в историю\n",
        "        total_score += score  # Обновляем суммарный счёт\n",
        "        mean_score = total_score / agent.n_games  # Считаем среднее значение очков\n",
        "        mean_scores.append(mean_score)  # Сохраняем среднее для графика\n",
        "        if score > agent.best_score:  # Если достигнут новый личный рекорд\n",
        "            agent.best_score = score  # Обновляем значение лучшего результата\n",
        "            torch.save(agent.model.state_dict(), config.best_model_path)  # Сохраняем веса модели как лучшую версию\n",
        "        plot_scores(scores, mean_scores)  # Обновляем графики Score/Mean Score\n",
        "        print(f\"Игра {agent.n_games}: Score={score} | Mean Score={mean_score:.2f} | Epsilon={agent.epsilon:.2f}\")  # Логируем номер игры, очки и текущее epsilon одним сообщением\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "create_ai_gif(  # Запускаем процедуру сохранения GIF с лучшим агентом\n",
        "    model_path=config.best_model_path,  # Загружаем лучшую модель по итогам обучения\n",
        "    config=config,  # Используем ту же конфигурацию игры, что и во время тренировки\n",
        "    gif_path=config.gif_path,  # Сохраняем GIF в путь, определённый в конфигурации\n",
        "    max_steps=config.gif_max_steps,  # Записываем ограниченное число кадров, чтобы ролик был компактным\n",
        "    duration=0.06,  # Устанавливаем длительность кадра в секундах (чем меньше, тем быстрее GIF)\n",
        ")  # Вызываем функцию создания GIF\n",
        "print(f\"GIF с игрой агента сохранён в файл: {config.gif_path}\")  # Сообщаем пользователю путь к GIF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Результаты\n",
        "\n",
        "- Агент обучается минимум на 200 играх, что удовлетворяет требованию (>=150).\n",
        "- Лучшая модель автоматически сохраняется в `best_flappy_q_agent.pth` и используется для генерации GIF.\n",
        "- График `Score`/`Mean Score` помогает оценить динамику обучения прямо во время тренировки.\n",
        "- Все параметры (learning rate, gamma, размеры слоёв, epsilon) подробно описаны в комментариях, а каждая строка кода снабжена пояснениями.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}